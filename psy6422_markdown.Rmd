---
title: "PSY6422 assignment"
author: "Ben"
date: "22/04/2022"
output: html_document
runtime: shiny
---

# **Tracking the spread - comparing two Covid 19 prevalence metrics across stages of the pandemic in England**

&nbsp;

## Research Question 

&nbsp;

To visualise the differences and similarities between the UK Government's data for people testing positive for Covid 19 and the ONS' (Office for National Statistics) modelled estimates for overall population prevalence. There are several key differences in how the two organisations collected this data; the ONS sent PCR tests to a random sample of the population weekly and used this to inform their modelled estimates of population prevalence. The Government's positive test data, meanwhile, relied heavily on self report, varied which tests were accepted as a positive result (i.e deciding whether to accept lateral flow results or not), and was less sensitive to asymptomatic cases as individuals usually only ordered a test if they thought they were infectious.

This can be summarised by saying the ONS tested a consistent sample consistently, using a more reliable tool (the PCR test), whereas the Government's positive test data came from an inconsistent sample being inconsistently tested.

I was also interested to see how the two metrics behaved over the different stages of the lockdown, for this reason only the data from England was analysed as the different nations of the UK had differing responses to the pandemic. Data from the Institute for Government was used to plot this, as they had compiled a timeline of the Government's response to the pandemic. 

Overall, the aim of this project was to visualise the estimated proportion of people in England who had Covid (aymptomatic or not), the proportion of those people actually testing positive (and reporting it!), and how these metrics varied over the stages of the response to the pandemic. 

&nbsp; 

## Data Origins

&nbsp;

[Government positive test data](https://coronavirus.data.gov.uk/details/cases).  
[ONS modelled estimates](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/bulletins/coronaviruscovid19infectionsurveypilot/18march2022).  
[Covid response timeline](https://www.instituteforgovernment.org.uk/charts/uk-government-coronavirus-lockdowns).

&nbsp;


## Data processing

&nbsp;

```{r Packages, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(here)
library(tidyverse)
library(ggplot2)
library(hablar)
library(npreg)
library(ggstance)
library(ggformula)
library(gridExtra)
```


```{r, results='hide'}
# Loading data
here()
onsfull <- read.csv(here::here("Data", "ons_c19_estimates_england.csv"),
                    skip = 5,
                    blank.lines.skip = TRUE)
gvtfull <- read.csv(here::here("Data", "gvtengland.csv"))

```

&nbsp;

Sample of the ONS initial data set. In this set, the items of interest to this project were "Fortnightly.weighted.estimates" and the "X.3" column, as this was the column for the overall  population prevalence estimate for the corresponding time period. 
&nbsp;

```{r echo=FALSE}
knitr::kable(head(onsfull))
```

&nbsp;

Government initial data set. This data set was formatted more conveniently, with "date" and "newCasesByPublishDate" being the items of interest.  


```{r, echo=FALSE}
knitr::kable(head(gvtfull))
```

&nbsp;

### Preparing ONS data

&nbsp;

```{r}
# Select relevant  data
onsfull <- onsfull[-c(10, 101:108), ]
onsselect <- onsfull %>% select(1, 5)
```

It was necessary to remove several rows from this data set, as the authors had included notes to inform the reader of methodological changes in how the population estimates were made. In terms of scalability, I would have preferred to use a loop to search the data automatically, however the format that the dates were entered combined with the non-uniform data (due to the included notes) made this quite complicated. I ultimately decided to remove these rows manually as it seemed to me to be the more time efficient option. 

&nbsp;

```{r, results='hide'}
# Separating the ONS dates into two separate columns
onsdates <- data.frame(str_split_fixed(onsselect$Fortnightly.weighted.estimates, " to ", n = 2))
onsdate1 <- as.Date(onsdates$X1, format = "%d %B %Y") 
onsdate2 <- as.Date(onsdates$X2, format = "%d %B %Y")

onsdf <- data.frame(onsdate1, onsdate2, onsselect$X.3, 
                    stringsAsFactors = TRUE) 
onsdf$date <- as.Date((onsdf$onsdate1 + ((onsdf$onsdate2 - onsdf$onsdate1) / 2)), 
                           format = "%d %B %Y")
```

Here I split the ONS date column into two columns, as this rendered the data into a format where I could properly assign them as 'datetime' data. I then calculated the 'average' of the two dates to get a single date I could assign the population estimate to, and subsequently plot.

&nbsp;

```{r}
onsdf <- na.omit(onsdf) # Remove NA values
onsdf[,'covid'] <- gsub(",","", onsdf[,'onsselect.X.3']) # Remove commas from X.3
rm(onsdates, onsdate1, onsdate2, onsfull, onsselect)
onsdf <- onsdf %>% convert(num(covid))
onsdf <- onsdf %>% select(4, 5)
```

&nbsp;

Sample of the final ONS data set:
&nbsp;

```{r, echo=FALSE}
knitr::kable(head(onsdf))
```

&nbsp;

### Preparing the Government data set

&nbsp;

```{r}
# Selecting relevant data
gvtfull <- gvtfull %>% filter(grepl('England', areaName))
gvtselect <- gvtfull %>% select(4, 5)

date <- as.Date(gvtselect$date, format = "%d/%m/%Y") 
covid <- gvtselect$newCasesByPublishDate
gvtdf <- data.frame(covid, date)
gvtdf$covid[gvtdf$covid==0] <- NA # Set 0 vals as NA
gvtdf <- na.omit(gvtdf) # Remove NA vals
rm(gvtfull, gvtselect, covid, date)
```

This set was much easier to prepare. The only real processing required was removing cases where 0 now cases were logged in a day. This was not due to no new cases actually occurring, but due to cases not being logged over the weekend. No data was lost by doing this, as cases occurring over the weekend were entered at the start of the subsequent week. This led to systematic spikes in the data which I will cover in more detail later on. 

&nbsp;

Sample of the final Government data set:

&nbsp;

```{r, echo=FALSE}
knitr::kable(head(gvtdf))
```

&nbsp;

### Institute for Government data

&nbsp;

As this data was qualitative (i.e descriptions of lockdown stringency rather than a numerical rating), there was no significant processing required. The stringency rating was visualised by vertical red bars on the plots, with higher stringency associated with higher opacity of red colouration. The scale ran from 0.0 - 0.5 (these being opacity values for the ggplot objects used). I created a data frame of these values for incorporation into the final plot later:

&nbsp;

```{r}
date1 <- as.Date(c("2020-05-03","2020-06-01","2020-06-15","2020-09-14","2020-11-05","2020-12-02",
                   "2021-01-06","2021-03-29","2021-04-12","2021-05-17","2021-07-19","2021-12-08"))
date2 <- as.Date(c("2020-06-01","2020-06-15","2020-09-14","2020-11-05","2020-12-02","2021-01-06",
                   "2021-03-29","2021-04-12","2021-05-17","2021-07-19","2021-12-08","2022-02-24"))
alpha <- c(0.5,0.4,0.3,0.3,0.5,0.4,0.5,0.4,0.3,0.2,0.1,0.2)
strindf <- data.frame(date1,date2,alpha)
rm(date1, date2, alpha)
```

&nbsp;

The criteria for rating were as follows:

0.5 (highest): This score was given to a full lockdown, where no-one except essential workers were permitted to leave the house for work.

0.4: Lockdown remains but with conditional allowances, for example; children being allowed to go back to school. The majority of the population is under heavy restriction, however.

0.3: Partial lifting; lockdown measures remain widely in place, but allowances now include most/the rest of the population. For example; non-essential businesses are still closed, but people can meet in groups of 6 outside.

0.2: Non-essential businesses/practises are allowed to open, however restrictions still remain such as having to eat outside or having restricted capacity.

0.1: Restrictions remain but disruption to life is minimal, examples include having to wear a mask to a restaurant but being able to take it off and eat inside once seated. 

0.0: Restrictions fully removed

&nbsp;

## First steps

&nbsp;

When building trial plots in preparation for the actual visualisation, it quickly became clear that it would be necessary to smooth this data in some way. As mentioned previously, the Government data set had large, systematic spikes in the data due to the fact that no new cases were processed over the weekend. This meant that every Monday had approximately triple the usual number of new cases. Due to the wide time period included in this visualisation, these large spikes quickly began to make it difficult to interpret the data.

&nbsp;

```{r, echo=FALSE}
ggp1 <- ggplot(NULL, aes(x = date, y = covid)) +  
    geom_line(data = gvtdf, col = "blue") +
  ylim(0, 200000)
ggp1
```


&nbsp;

As can be seen, the data gets quite hard to accurately interpret. The large spikes naturally draw the eye to the top op the peak and make the Covid levels look much higher than they actually are. The challenge here was that none of the usual methods of smoothing seemed to be appropriate, usually because they relied on some form of linear formula. Fortunately, this could be dealt with using a spline - a piece-wise polynomial regression which separates the data into bins, running the regression between them. The more bins specified the closer the spline fits the original data.

&nbsp;

```{r, echo=FALSE}
ggp1 <- ggplot(NULL, aes(x = date, y = covid)) +  
    geom_line(data = gvtdf, col = "blue") +
  ylim(0, 200000)
ggp2 <- ggplot(NULL, aes(x = date, y = covid)) +
  geom_spline(data = gvtdf, 
              aes(x = date, y = covid), nknots = 60) +
  ylim(0, 200000)
ggpubr::ggarrange(ggp1, ggp2, ncol = 2)
```

&nbsp;

Much better! With this done, I could get to work on the first visualisation. 

&nbsp;

## Visualisation 1

&nbsp;

The first visualisation I wanted to try was a dual Y axis graph. This was primarily to see in detail how the two metrics varied across the different stages of lockdown. By plotting them together I aimed to provide the simplest comparison. 

&nbsp;

```{r, warning=FALSE, message=FALSE, fig.width=12, fig.height=8}
#Creating function to add stringency bars
barfunction <- function(date1, date2, alpha){
  a <- annotate(geom = "rect",
                xmin = as.Date(date1), xmax = as.Date(date2), ymin = 0, ymax = Inf, 
                alpha = alpha, fill = "red")
  return(a)
}

##Building graph

coeff <- 0.05

#Adding initial data
compggp <- ggplot(NULL, aes(x = date, y = covid)) +
  geom_spline(data = onsdf, 
              aes(x = date, y = covid, colour = "ONS Modeled Estimates"), nknots = 90, size = 1.3) +
  geom_spline(data = gvtdf, 
              aes(x = date, y = covid/coeff, colour = "Gvt Reported Positive Tests"), nknots = 90, size = 1.3)

#Adding stringency bars
compggp <- compggp + purrr::pmap(strindf, barfunction) 

#Adding aesthetics
compggp <- compggp + scale_x_date(limits = as.Date(c("2020-05-03", NA ))) +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Date (year - month)", y = "Covid Cases (estimated and reported)") + 
  scale_y_continuous(labels = scales::comma, name = "Modeled Population Estimates", 
                     sec.axis = sec_axis(~.*coeff, name = "Reported Positive Tests")) +
  labs(title = "Estimated vs Reported Covid Cases over Lockdown", 
       subtitle = "Data sourced from ONS, UK Government and the Institute for Government", 
       x = "Date (year - month)", y = "Covid Cases (estimated and reported)") +
  scale_colour_manual(name = "",
                      values = c("ONS Modeled Estimates"="khaki4", 
                                 "Gvt Reported Positive Tests" = "darkcyan",
                                 "Lockdown Stringency" = "red"))
compggp

#Save graph
ggsave("comp_y_6422.pdf", path = here::here("Figures"))

```

&nbsp;

It is important to note that this graph is NOT intended to provide an accurate comparison of the extent of Covid infection in the population, as the dual Y axis is very misleading in this regard. However, given the scale of the numerical difference in estimates I thought that it would be useful. 

&nbsp;

As we can see from the graph, at first glance there do seem to be some similarities between the metrics, with similar gradients across the timeline. Whilst, as mentioned, the dual axis is misleading in terms of overall estimates, the metrics seem to covary reasonably consistently across the various stages of the pandemic. This graph type was handy for visualising the data for both metrics whilst retaining a high level of detail for both.

&nbsp;

## Visualisation 2

&nbsp;

The second plot was a composite line graph. The benefit of this style of visualisation is that it gives a far more immediately representative visual comparison of the differences in scale between the two metrics.

&nbsp;

```{r, warning=FALSE, message=FALSE, fig.width=12, fig.height=8}
##Comp line graph
#Adding initial data
ggp <- ggplot(NULL, aes(x = date, y = covid)) + 
  geom_spline(data = onsdf, 
              aes(x = date, y = covid, colour = "ONS Modeled Estimates"), nknots = 90, size = 1.3) +
  geom_spline(data = gvtdf, 
              aes(x = date, y = covid, colour = "Gvt Reported Positive Tests"), nknots = 90, size = 1.3) 


#Adding lockdown stringency bars
ggp <- ggp + purrr::pmap(strindf, barfunction) 


#Adding aesthetics
ggp <- ggp + labs(title = "Estimated vs Reported Covid Cases over lockdown", 
       subtitle = "Data sourced from ONS, UK Government and Institute for Government", 
       x = "Date (year - month)", y = "Covid Cases") +
  scale_y_continuous(labels = scales::comma) +
  scale_x_date(limits = as.Date(c("2020-05-03", NA ))) +
  scale_colour_manual(name = "Legend",
                      values = c("ONS Modeled Estimates"="khaki4", 
                                 "Gvt Reported Positive Tests" = "darkcyan",
                                 "Lockdown Stringency"="red"))
ggp
#Save graph
ggsave("comp_line_6422.pdf", path = here::here("Figures"))

```

&nbsp;

The composite graph really shows the main difference between the two metrics. Whilst it can be seen from visualisation 1 that the reported covid tests also vary over the course of the lockdown, the sheer scale of the difference between the reported tests and the estimated total number of cases is quite impressive (of course this is assuming the modeled total is 100% accurate which is unlikely). 

When taking both graphs into account, I find it interesting to see the effect the lockdowns had, with especially the most stringent measures having a noticeable correlation with a falling gradient in estimated covid prevalence.

Of interest as well was the possible association with the end of December/beginning of new year with significantly rising levels of infection.

&nbsp;

## Thoughts

&nbsp;

Whilst both of these visualisations had useful aspects, they both had drawbacks as well. I wasn't happy with the dual y axis due to it not providing a valid visual comparison of the scale of the differences. However, the composite line graph with a single axis, whilst providing a very satisfying comparison of the difference in scale, basically rendered the government data set illegible. So, how to combine the detail of the dual axis graph with the scale of the single axis?

## Final visualisation 

```{r, message=FALSE, warning=FALSE, fig.width=12, fig.height=10}
###Shiny and chrome

library(shiny)
library(ggplot2)

#Initial layout

ui <- fluidPage(
  titlePanel("Line Graph"),
  sidebarLayout(
    sidebarPanel(
      sliderInput("scalegvt","Scale Government Data by:",  min = 1.0, max = 25, value = c(1.0)),
      sliderInput("scaleons", "Scale ONS Data by:", min = 0.05, max = 1.0, value = c(1.0))
    ),
    
    mainPanel(
      plotOutput("distPlot")
    )
  )
)

## Creating server 
server <- function(input, output) {
  coeff1 <- reactive({input$scalegvt 
  })
  coeff2 <- reactive({input$scaleons
    })
  output$distPlot <- renderPlot({
    compggp <- ggplot(NULL, aes(x = date, y = covid)) +
      geom_spline(data = onsdf, 
                  aes(x = date, y = covid*coeff2(), colour = "ONS Modelled Estimates"), nknots = 90, size = 1.3) +
      geom_spline(data = gvtdf, 
                  aes(x = date, y = covid*coeff1(), colour = "Gvt Reported Positive Tests"), nknots = 90, 
                  size = 1.3)
    
    #Stringency bars
    compggp <- compggp + purrr::pmap(strindf, barfunction)
    
    #Adding aesthetics
    compggp <- compggp + scale_x_date(limits = as.Date(c("2020-05-03", NA ))) +
      theme_minimal() +
      scale_y_continuous(labels = scales::comma) +
      labs(x = "Date (year - month)", y = "Covid Cases (estimated/reported)") + 
      scale_y_continuous(labels = scales::comma) +
      scale_colour_manual(name = "",
                          values = c("ONS Modelled Estimates"="khaki4", 
                                     "Gvt Reported Positive Tests" = "darkcyan",
                                     "Lockdown Stringency" = "red"))
    compggp
  })
}

# Running application
shinyApp(ui = ui, server = server)

```


## Summary

